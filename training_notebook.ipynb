{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1cdea-eadb-4a68-9ef5-5a755558b398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13157fd-fd8a-4d1f-8315-3f705df3d00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.26.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/contrastive-image-text/requirements.txt\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    feature_extractor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_vision_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the vision model parameters or not.\"}\n",
    "    )\n",
    "    freeze_text_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the text model parameters or not.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n",
    "    image_column: Optional[str] = field(\n",
    "        default=\"image_path\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n",
    "    )\n",
    "    caption_column: Optional[str] = field(\n",
    "        default=\"caption\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension == \"json\", \"`validation_file` should be a json file.\"\n",
    "\n",
    "\n",
    "dataset_name_mapping = {\n",
    "    \"image_caption_dataset.py\": (\"image_path\", \"caption\"),\n",
    "}\n",
    "\n",
    "\n",
    "# We use torchvision for faster image pre-processing. The transforms are implemented as nn.Module,\n",
    "# so we jit it to be faster.\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def main(input_str):\n",
    "    # 1. Parse input arguments\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(input_str.split())\n",
    "\n",
    "    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "    send_example_telemetry(\"run_clip\", model_args, data_args)\n",
    "\n",
    "    # 2. Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # 3. Detecting last checkpoint and eventualy continue from last checkpoint\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # 4. Load dataset\n",
    "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # For CSV/JSON files this script will use the first column for the full image path and the second column for the\n",
    "    # captions (unless you specify column names for this with the `image_column` and `caption_column` arguments).\n",
    "    #\n",
    "    if data_args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        dataset = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            keep_in_memory=False,\n",
    "            data_dir=data_args.data_dir,\n",
    "        )\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if data_args.train_file is not None:\n",
    "            data_files[\"train\"] = data_args.train_file\n",
    "            extension = data_args.train_file.split(\".\")[-1]\n",
    "        if data_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = data_args.validation_file\n",
    "            extension = data_args.validation_file.split(\".\")[-1]\n",
    "        if hasattr(data_args, 'test_file') and data_args.test_file is not None:\n",
    "            data_files[\"test\"] = data_args.test_file\n",
    "            extension = data_args.test_file.split(\".\")[-1]\n",
    "        dataset = load_dataset(\n",
    "            extension,\n",
    "            data_files=data_files,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # 5. Load pretrained model, tokenizer, and feature extractor\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "        )\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "    # Load feature_extractor, in this script we only use this to get the mean and std for normalization.\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "        model_args.feature_extractor_name or model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "    )\n",
    "\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "    )\n",
    "    config = model.config\n",
    "\n",
    "    def _freeze_params(module):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if model_args.freeze_vision_model:\n",
    "        _freeze_params(model.vision_model)\n",
    "\n",
    "    if model_args.freeze_text_model:\n",
    "        _freeze_params(model.text_model)\n",
    "\n",
    "    # set seed for torch dataloaders\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize inputs and targets.\n",
    "    if training_args.do_train:\n",
    "        column_names = dataset[\"train\"].column_names\n",
    "    elif training_args.do_eval:\n",
    "        column_names = dataset[\"validation\"].column_names\n",
    "    elif training_args.do_predict:\n",
    "        column_names = dataset[\"test\"].column_names\n",
    "    else:\n",
    "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
    "        return\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)\n",
    "    if data_args.image_column is None:\n",
    "        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "    else:\n",
    "        image_column = data_args.image_column\n",
    "        if image_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "            )\n",
    "    if data_args.caption_column is None:\n",
    "        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "    else:\n",
    "        caption_column = data_args.caption_column\n",
    "        if caption_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "            )\n",
    "\n",
    "    # 7. Preprocessing the datasets.\n",
    "    # Initialize torchvision transforms and jit it for faster processing.\n",
    "    image_transformations = Transform(\n",
    "        config.vision_config.image_size, feature_extractor.image_mean, feature_extractor.image_std\n",
    "    )\n",
    "    image_transformations = torch.jit.script(image_transformations)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = [caption for caption in examples[caption_column]]\n",
    "        text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "    def transform_images(examples):\n",
    "        images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        return examples\n",
    "\n",
    "    def filter_corrupt_images(examples):\n",
    "        \"\"\"remove problematic images\"\"\"\n",
    "        valid_images = []\n",
    "        for image_file in examples[image_column]:\n",
    "            try:\n",
    "                Image.open(image_file)\n",
    "                valid_images.append(True)\n",
    "            except Exception:\n",
    "                valid_images.append(False)\n",
    "        return valid_images\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in dataset:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "        train_dataset = train_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        train_dataset = train_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "\n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        train_dataset.set_transform(transform_images)\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if \"validation\" not in dataset:\n",
    "            raise ValueError(\"--do_eval requires a train validation\")\n",
    "        eval_dataset = dataset[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "\n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "\n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        eval_dataset.set_transform(transform_images)\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        if \"test\" not in dataset:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        test_dataset = dataset[\"test\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(test_dataset), data_args.max_eval_samples)\n",
    "            test_dataset = test_dataset.select(range(max_eval_samples))\n",
    "\n",
    "        test_dataset = test_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        test_dataset = test_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on test dataset\",\n",
    "        )\n",
    "\n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        test_dataset.set_transform(transform_images)\n",
    "\n",
    "    # 8. Initalize our trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "\n",
    "    # 9. Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        trainer.save_model()\n",
    "        trainer.log_metrics(\"train\", train_result.metrics)\n",
    "        trainer.save_metrics(\"train\", train_result.metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # 10. Evaluation\n",
    "    if training_args.do_eval:\n",
    "        metrics = trainer.evaluate()\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    # 11. Write Training Stats and push to hub.\n",
    "    kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"contrastive-image-text-modeling\"}\n",
    "    if data_args.dataset_name is not None:\n",
    "        kwargs[\"dataset_tags\"] = data_args.dataset_name\n",
    "        if data_args.dataset_config_name is not None:\n",
    "            kwargs[\"dataset_args\"] = data_args.dataset_config_name\n",
    "            kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n",
    "        else:\n",
    "            kwargs[\"dataset\"] = data_args.dataset_name\n",
    "\n",
    "    if training_args.push_to_hub:\n",
    "        trainer.push_to_hub(**kwargs)\n",
    "    else:\n",
    "        trainer.create_model_card(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff69c6-18af-46a5-aa89-420ceba0090f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_id =  \"patrickjohncyh/fashion-clip\"\n",
    "output_folder = \"./clip-finetuned_patrick_b512_lowlr\"\n",
    "out_json = \"./cods_dataset_CLIP.json\"\n",
    "batch_size = 512\n",
    "num_train_epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fddb2-ad6b-4589-bd58-a5539dbc500d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_arg_str = (\n",
    "    '--output_dir ' + str(output_folder) + \n",
    "    ' --model_name_or_path ' + str(repo_id) + \n",
    "    ' --train_file ' + str(out_json) + \n",
    "    ' --image_column image ' + \n",
    "    ' --overwrite_output_dir=True ' + \n",
    "    ' --max_seq_length=77 ' + \n",
    "    ' --num_train_epochs=' + str(num_train_epochs) + \n",
    "    ' --caption_column caption ' + \n",
    "    ' --remove_unused_columns=False ' + \n",
    "    ' --do_train ' + \n",
    "    ' --per_device_train_batch_size=' + str(batch_size) + \n",
    "    ' --learning_rate=0.00002 ' + \n",
    "    ' --warmup_steps=0 ' + \n",
    "    ' --weight_decay 0.1 ' + \n",
    "    ' --save_steps=2000 ' + \n",
    "    ' --save_strategy=epoch ' + \n",
    "    ' --save_total_limit=6'\n",
    ")\n",
    "\n",
    "input_arg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236125db-5df8-4af6-bfd1-66b00250635d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main(input_arg_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
